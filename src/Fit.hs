{-# LANGUAGE GeneralizedNewtypeDeriving #-}
module Fit (Program(..), testFit) where

import Prelude

import           Data.Function
import           Data.List (nubBy, sort)
import qualified Data.Vector.Generic as V
import qualified Data.Vector.Unboxed
import           Test.QuickCheck.Arbitrary
import           Test.QuickCheck.Gen
import           Test.QuickCheck.Random

import AD

type Seed = Int

-- These types need to have infinitely many values
-- and exact and cheap enough Eq relative to the bound on sizes
-- of individual values as generated by @rollSamples@.
type X = Int

type Y = Int

-- | Sample inputs and outputs that all programs have to satisfy.
type Samples = [(X, Y)]

-- | Programs over discrete types (integers, lists, etc.) taking
-- input from @X@ and producing @Y@.
data Program = P
  deriving (Show, Eq)

type Params = Data.Vector.Unboxed.Vector Float

-- | Produce a program based on samples and parameters.
construct :: Samples -> Params -> Program
construct _samples _params = P

-- | How much the program differs from the ideal, which is an empty program
-- that nevertheless satisfies all samples.
-- This is a fixed weighted sum of: number of samples not satisfied,
-- code size, sum of runtime over all samples (physical or ticks),
-- sum of heap allocation over all samples (physical or simplified).
-- If the program runs out of time or heap space and is terminated,
-- we assume it fails for that sample.
loss :: Samples -> Program -> Float
loss _samples _p = 1

step :: Samples -> Params -> (Program, Float)
step samples params =
  let p = construct samples params
      l = loss samples p
  in (p, l)

-- Above this line, the code needs to be differentiable over params.
--------------------------------------------------------------------

-- | Randomly generate samples.
-- Both xs and ys need to be small, perhaps bounded,
-- to ensure not too much RAM and runtime taken.
-- Generate up to @n@ pairs and more than half that on average.
--
-- Using quickeck machinery, because it can cope with arbitrary types easily.
rollSamples :: Seed -> Int -> Samples
rollSamples seed n =
  let generated = [unGen arbitrary (mkQCGen $ seed + k) (k * 2) | k <- [0 .. n]]
        -- TODO: check if splitting is a better idea
        -- (System.Random.SplitMix import would be needed)
  in nubBy ((==) `on` fst) $ sort generated

train :: Seed -> Int -> Int -> Int -> Params -> Params
train seed nSessions nIterations nSamples paramsInitial =
  go nSessions paramsInitial
 where
  go :: Int -> Params -> Params
  go 0 !params = params
  go n params =
    let samples = rollSamples (seed + n) nSamples
        f params1 = snd $ step samples params1
          -- TODO: add a deep neural network here into the mix and best
          -- if it somehow emerges naturally or if something better emerges
        params2 = gradDesc 0.1 undefined {-f-} nIterations params
    in go (pred n) params2

testFit :: Seed -> Int -> Int -> Int -> Int -> (Program, Samples, Int)
testFit seed nSessions nIterations nSamples nParams =
  let paramsInitial = V.replicate nParams 0.23
      params2 = train seed nSessions nIterations nSamples paramsInitial
      samples = rollSamples (seed - 1) nSamples
      p = construct samples params2
  in (p, samples, length samples)

-- For this to work, I need to be able to differentiate over Program,
-- which is a problem, because it's a recursive and discrete type,
-- with no real numbers in it.
-- I may need to somehow smoothen the functions, so that some
-- conditionals will have 0.324 tendency to become recursions
-- (and so incurring higher loss, too) and others
-- a -0.21 attraction towards removing the second branch
-- and vanishing (lowering loss due to code size and runtime,
-- but incurring a risk of not satisfying an additional sample).




























-- generalize to be able to bootstrap meaningfully
